{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3399c4-809f-48eb-84b6-74ee19950573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform my imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import heapq\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import Optional, List\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "import json, csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Add the certs information\n",
    "sys.path.append(\"..\")\n",
    "from CeRTS_beam_multi import *\n",
    "from CeRTS_utils import *\n",
    "sys.path.remove(\"..\")\n",
    "\n",
    "# Add some fine-tuning specific functions\n",
    "from functions.generation_functions import *\n",
    "from functions.dataset_creation_functions import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e27b2-565f-456f-84a8-3fa264e26b7b",
   "metadata": {},
   "source": [
    "# Data Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d93fc-b448-4b2e-9ea6-9d8933d9a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(\"clinical_data/validation.csv\")\n",
    "\n",
    "json_template = {\n",
    "    \"asthma\": \"yes|no\",\n",
    "    \"smoking\": \"yes|no\",\n",
    "    \"pneu\": \"yes|no\",            \n",
    "    \"common_cold\": \"yes|no\",\n",
    "    \"pain\": \"yes|no\",\n",
    "    \"fever\": \"high|low|no\",\n",
    "    \"antibiotics\": \"yes|no\"\n",
    "}\n",
    "features = json_template.keys()\n",
    "\n",
    "# add gold-standard output as text to df\n",
    "df_val = create_gold_standard_extractions(df_val, json_template)\n",
    "\n",
    "# Split into training and testing for temporary stuff \n",
    "df_val_train, df_val_test = train_test_split(df_val, test_size=0.2, random_state=42)\n",
    "df_val_train.reset_index(drop=True, inplace = True)\n",
    "df_val_test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f5fc7-00aa-4fa8-a860-54d91616c963",
   "metadata": {},
   "source": [
    "# Load in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51855f-5fdd-4f7b-a882-af58c96ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load model (using float16 + device map for GPU efficiency)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fd411-d8f3-4185-85ef-f2ef612a01d4",
   "metadata": {},
   "source": [
    "# Dataset in format for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafb78e-a1df-4a49-9ba6-94b447059b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Step\n",
    "train_data = format_for_standard_SFT(df_val_train, json_template)\n",
    "\n",
    "# Put in List\n",
    "train_data = [\n",
    "    {\"messages\": conv} for conv in train_data   # conv is your list of role/content dicts\n",
    "]\n",
    "\n",
    "# Get in datasets\n",
    "train_data = Dataset.from_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7eb66-56d1-4c01-8259-32e556a4e180",
   "metadata": {},
   "source": [
    "# Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba495e4-3bd2-4bdf-8852-10f94489d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,                        # low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# ---- Step 4: Training arguments ----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{model_path}-SFT-Levi-test',\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "# ---- Step 5: Trainer with PEFT/LoRA ----\n",
    "updated_data = [\n",
    "    {\"messages\": conv} for conv in train_data   # conv is your list of role/content dicts\n",
    "]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config   # <<< LoRA here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ca30f-eef6-4587-a3e9-db43d32614ff",
   "metadata": {},
   "source": [
    "# Actually Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43c2b99-12a3-45b4-ba07-40bbd7621d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/llama-3.2-3b-instruct-SFT-Levi-test-merged/tokenizer_config.json',\n",
       " 'models/llama-3.2-3b-instruct-SFT-Levi-test-merged/special_tokens_map.json',\n",
       " 'models/llama-3.2-3b-instruct-SFT-Levi-test-merged/chat_template.jinja',\n",
       " 'models/llama-3.2-3b-instruct-SFT-Levi-test-merged/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Then Save\n",
    "trainer.model.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "print(f\"âœ… LoRA adapter saved to {training_args.output_dir}\")\n",
    "\n",
    "# Save merged Model (so don't have to marge and unload when using later)\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "final_model_dir = f\"{training_args.output_dir}-merged\"\n",
    "merged_model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9da7a-0a49-4362-bef6-8875e257ded9",
   "metadata": {},
   "source": [
    "# Now mess around with my data just to get used to transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd51a8-6b69-4e26-9cfc-3438c8b1929b",
   "metadata": {},
   "source": [
    "### Load in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4828c33-e894-45e0-9839-9df4575a1528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0f0db5873041e6b48ef3087ebaf025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models/llama-3.2-3b-instruct\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load model (using float16 + device map for GPU efficiency)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09033850-a3c7-4d95-90b0-472044ea5be3",
   "metadata": {},
   "source": [
    "### Now Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3de8f1bb-5485-47ef-afe3-ca4235281705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"asthma\": \"no\",\"smoking\": \"no\",\"pneu\": \"no\",\"common_cold\": \"yes\",\"pain\": \"no\",\"fever\": \"no\",\"antibiotics\": \"no\"}<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 14\n",
    "\n",
    "# Model\n",
    "df_val.loc[n, 'advanced_text']\n",
    "prompt = gen_prompt_no_shot(df_val.loc[n, 'advanced_text'], features, json_template)\n",
    "\n",
    "# convert messages into a proper model input\n",
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# decode to string\n",
    "generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "response = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "# \n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29ac70-2933-42e9-bf9d-9a75de4f1a0a",
   "metadata": {},
   "source": [
    "### Make Our Training Arguments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
